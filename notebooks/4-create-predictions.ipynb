{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras as kr\n",
    "import pathlib as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/blanc/Documents/GitHub/titanic-ml-tfpj/data/raw/test.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model_path = pl.Path(\"../models/inference_model.keras\")\n",
    "training_model_path = pl.Path(\"../models/training_model.keras\")\n",
    "titanic_data_test_path = pl.Path(\"../data/raw/test.csv\")\n",
    "\n",
    "inference_model_path.resolve()\n",
    "training_model_path.resolve()\n",
    "titanic_data_test_path.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se deben predecir un total de 418 casos\n"
     ]
    }
   ],
   "source": [
    "titanic_data_test = pd.read_csv(filepath_or_buffer=titanic_data_test_path)\n",
    "titanic_data_test.loc[titanic_data_test[\"Embarked\"].isna(), \"Embarked\"] = \"D\"\n",
    "titanic_data_test = titanic_data_test.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"Pclass\",\n",
    "        \"Sex\",\n",
    "        \"Age\",\n",
    "        \"SibSp\",\n",
    "        \"Parch\",\n",
    "        \"Fare\",\n",
    "        \"Embarked\",\n",
    "    ],\n",
    "]\n",
    "print(\"Se deben predecir un total de {} casos\".format(len(titanic_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se deben predecir un total de 1 casos\n"
     ]
    }
   ],
   "source": [
    "# input_dict = {name: tf.convert_to_tensor(value) for name, value in sample.items()}\n",
    "titanic_dataset = tf.data.Dataset.from_tensors(tensors=dict(titanic_data_test))\n",
    "\n",
    "print(\n",
    "    \"Se deben predecir un total de {} casos\".format(\n",
    "        len(titanic_dataset),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Pclass': <tf.Tensor: shape=(418,), dtype=int64, numpy=\n",
       "  array([3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 2, 2, 3, 3, 3, 1, 3,\n",
       "         1, 1, 1, 3, 1, 3, 1, 3, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 2,\n",
       "         1, 3, 1, 3, 1, 3, 1, 2, 2, 1, 2, 3, 3, 3, 3, 1, 3, 2, 3, 3, 1, 2,\n",
       "         3, 1, 1, 1, 3, 3, 3, 1, 1, 1, 3, 1, 2, 3, 3, 1, 1, 3, 2, 3, 3, 3,\n",
       "         3, 2, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "         2, 3, 1, 3, 1, 3, 3, 3, 1, 2, 2, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 1,\n",
       "         3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1, 2, 1, 3, 3, 3,\n",
       "         3, 3, 1, 3, 1, 3, 3, 3, 2, 3, 2, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 2,\n",
       "         2, 1, 2, 1, 2, 1, 1, 3, 1, 2, 2, 3, 3, 2, 2, 1, 3, 2, 2, 3, 1, 3,\n",
       "         2, 3, 3, 3, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 2, 2, 3, 1, 3, 1, 1, 3,\n",
       "         2, 3, 2, 3, 1, 3, 3, 3, 3, 2, 2, 1, 3, 3, 1, 3, 1, 3, 2, 1, 1, 2,\n",
       "         1, 3, 3, 1, 2, 2, 2, 3, 2, 3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3,\n",
       "         2, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "         3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 2, 2, 3, 3, 1, 1, 3,\n",
       "         1, 3, 3, 3, 3, 3, 1, 3, 1, 2, 3, 2, 3, 3, 2, 1, 1, 3, 2, 1, 2, 2,\n",
       "         2, 1, 3, 3, 3, 1, 2, 3, 2, 3, 2, 3, 3, 1, 3, 3, 2, 3, 2, 2, 1, 2,\n",
       "         2, 2, 3, 1, 1, 3, 3, 3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 2, 2, 1, 1, 2,\n",
       "         1, 1, 3, 2, 1, 3, 3, 3, 3, 3, 2, 2, 3, 2, 3, 3, 1, 1, 3, 2, 3, 1,\n",
       "         3, 1, 3, 3, 1, 2, 1, 1, 1, 2, 2, 1, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3],\n",
       "        dtype=int64)>,\n",
       "  'Sex': <tf.Tensor: shape=(418,), dtype=string, numpy=\n",
       "  array([b'male', b'female', b'male', b'male', b'female', b'male',\n",
       "         b'female', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'female', b'male', b'female', b'female', b'male', b'male',\n",
       "         b'female', b'female', b'male', b'male', b'female', b'male',\n",
       "         b'female', b'male', b'female', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'female', b'female', b'male', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'female', b'female',\n",
       "         b'male', b'male', b'female', b'female', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'female', b'male', b'male', b'male', b'female',\n",
       "         b'male', b'female', b'female', b'male', b'male', b'female',\n",
       "         b'female', b'male', b'female', b'male', b'female', b'male',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'male', b'female', b'female', b'female',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'female', b'male', b'female', b'male', b'female', b'male',\n",
       "         b'male', b'male', b'female', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'female', b'female', b'female', b'female',\n",
       "         b'male', b'male', b'female', b'male', b'female', b'female',\n",
       "         b'male', b'female', b'male', b'male', b'female', b'male',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'female', b'male',\n",
       "         b'male', b'male', b'male', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'female', b'male', b'male', b'female', b'male',\n",
       "         b'male', b'female', b'female', b'male', b'female', b'female',\n",
       "         b'male', b'female', b'male', b'male', b'female', b'male', b'male',\n",
       "         b'female', b'female', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'female', b'female', b'male', b'female', b'female', b'male',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'female',\n",
       "         b'male', b'female', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'female', b'male', b'male',\n",
       "         b'female', b'male', b'female', b'male', b'male', b'male', b'male',\n",
       "         b'female', b'female', b'male', b'female', b'male', b'female',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'female', b'female', b'female', b'female', b'male', b'male',\n",
       "         b'male', b'male', b'female', b'male', b'female', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'female', b'male', b'male', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'male', b'female', b'male',\n",
       "         b'male', b'male', b'female', b'female', b'male', b'female',\n",
       "         b'male', b'male', b'male', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'female', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'female', b'male', b'male', b'male', b'male', b'female',\n",
       "         b'male', b'male', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'female', b'female', b'male', b'male', b'male', b'female',\n",
       "         b'male', b'male', b'male', b'female', b'female', b'female',\n",
       "         b'male', b'male', b'male', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'female', b'male', b'male', b'female', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'male', b'male', b'male', b'male', b'female',\n",
       "         b'male', b'female', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'male', b'female', b'male',\n",
       "         b'female', b'male', b'male', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'female', b'female', b'male', b'female',\n",
       "         b'female', b'male', b'male', b'female', b'male', b'male',\n",
       "         b'female', b'female', b'female', b'male', b'male', b'male',\n",
       "         b'male', b'male', b'female', b'female', b'male', b'female',\n",
       "         b'male', b'male', b'male', b'male', b'male', b'female', b'male',\n",
       "         b'male', b'male', b'female', b'male', b'female', b'male', b'male',\n",
       "         b'female', b'male', b'female', b'male', b'male', b'male', b'male',\n",
       "         b'male', b'female', b'female', b'female', b'female', b'female',\n",
       "         b'male', b'female', b'male', b'male', b'male'], dtype=object)>,\n",
       "  'Age': <tf.Tensor: shape=(418,), dtype=float64, numpy=\n",
       "  array([34.5 , 47.  , 62.  , 27.  , 22.  , 14.  , 30.  , 26.  , 18.  ,\n",
       "         21.  ,   nan, 46.  , 23.  , 63.  , 47.  , 24.  , 35.  , 21.  ,\n",
       "         27.  , 45.  , 55.  ,  9.  ,   nan, 21.  , 48.  , 50.  , 22.  ,\n",
       "         22.5 , 41.  ,   nan, 50.  , 24.  , 33.  ,   nan, 30.  , 18.5 ,\n",
       "           nan, 21.  , 25.  ,   nan, 39.  ,   nan, 41.  , 30.  , 45.  ,\n",
       "         25.  , 45.  ,   nan, 60.  , 36.  , 24.  , 27.  , 20.  , 28.  ,\n",
       "           nan, 10.  , 35.  , 25.  ,   nan, 36.  , 17.  , 32.  , 18.  ,\n",
       "         22.  , 13.  ,   nan, 18.  , 47.  , 31.  , 60.  , 24.  , 21.  ,\n",
       "         29.  , 28.5 , 35.  , 32.5 ,   nan, 55.  , 30.  , 24.  ,  6.  ,\n",
       "         67.  , 49.  ,   nan,   nan,   nan, 27.  , 18.  ,   nan,  2.  ,\n",
       "         22.  ,   nan, 27.  ,   nan, 25.  , 25.  , 76.  , 29.  , 20.  ,\n",
       "         33.  , 43.  , 27.  ,   nan, 26.  , 16.  , 28.  , 21.  ,   nan,\n",
       "           nan, 18.5 , 41.  ,   nan, 36.  , 18.5 , 63.  , 18.  ,   nan,\n",
       "          1.  , 36.  , 29.  , 12.  ,   nan, 35.  , 28.  ,   nan, 17.  ,\n",
       "         22.  ,   nan, 42.  , 24.  , 32.  , 53.  ,   nan,   nan, 43.  ,\n",
       "         24.  , 26.5 , 26.  , 23.  , 40.  , 10.  , 33.  , 61.  , 28.  ,\n",
       "         42.  , 31.  ,   nan, 22.  ,   nan, 30.  , 23.  ,   nan, 60.5 ,\n",
       "         36.  , 13.  , 24.  , 29.  , 23.  , 42.  , 26.  ,   nan,  7.  ,\n",
       "         26.  ,   nan, 41.  , 26.  , 48.  , 18.  ,   nan, 22.  ,   nan,\n",
       "         27.  , 23.  ,   nan, 40.  , 15.  , 20.  , 54.  , 36.  , 64.  ,\n",
       "         30.  , 37.  , 18.  ,   nan, 27.  , 40.  , 21.  , 17.  ,   nan,\n",
       "         40.  , 34.  ,   nan, 11.5 , 61.  ,  8.  , 33.  ,  6.  , 18.  ,\n",
       "         23.  ,   nan,   nan,  0.33, 47.  ,  8.  , 25.  ,   nan, 35.  ,\n",
       "         24.  , 33.  , 25.  , 32.  ,   nan, 17.  , 60.  , 38.  , 42.  ,\n",
       "           nan, 57.  , 50.  ,   nan, 30.  , 21.  , 22.  , 21.  , 53.  ,\n",
       "           nan, 23.  ,   nan, 40.5 , 36.  , 14.  , 21.  , 21.  ,   nan,\n",
       "         39.  , 20.  , 64.  , 20.  , 18.  , 48.  , 55.  , 45.  , 45.  ,\n",
       "           nan,   nan, 41.  , 22.  , 42.  , 29.  ,   nan,  0.92, 20.  ,\n",
       "         27.  , 24.  , 32.5 ,   nan,   nan, 28.  , 19.  , 21.  , 36.5 ,\n",
       "         21.  , 29.  ,  1.  , 30.  ,   nan,   nan,   nan,   nan, 17.  ,\n",
       "         46.  ,   nan, 26.  ,   nan,   nan, 20.  , 28.  , 40.  , 30.  ,\n",
       "         22.  , 23.  ,  0.75,   nan,  9.  ,  2.  , 36.  ,   nan, 24.  ,\n",
       "           nan,   nan,   nan, 30.  ,   nan, 53.  , 36.  , 26.  ,  1.  ,\n",
       "           nan, 30.  , 29.  , 32.  ,   nan, 43.  , 24.  ,   nan, 64.  ,\n",
       "         30.  ,  0.83, 55.  , 45.  , 18.  , 22.  ,   nan, 37.  , 55.  ,\n",
       "         17.  , 57.  , 19.  , 27.  , 22.  , 26.  , 25.  , 26.  , 33.  ,\n",
       "         39.  , 23.  , 12.  , 46.  , 29.  , 21.  , 48.  , 39.  ,   nan,\n",
       "         19.  , 27.  , 30.  , 32.  , 39.  , 25.  ,   nan, 18.  , 32.  ,\n",
       "           nan, 58.  ,   nan, 16.  , 26.  , 38.  , 24.  , 31.  , 45.  ,\n",
       "         25.  , 18.  , 49.  ,  0.17, 50.  , 59.  ,   nan,   nan, 30.  ,\n",
       "         14.5 , 24.  , 31.  , 27.  , 25.  ,   nan,   nan, 22.  , 45.  ,\n",
       "         29.  , 21.  , 31.  , 49.  , 44.  , 54.  , 45.  , 22.  , 21.  ,\n",
       "         55.  ,  5.  ,   nan, 26.  ,   nan, 19.  ,   nan, 24.  , 24.  ,\n",
       "         57.  , 21.  ,  6.  , 23.  , 51.  , 13.  , 47.  , 29.  , 18.  ,\n",
       "         24.  , 48.  , 22.  , 31.  , 30.  , 38.  , 22.  , 17.  , 43.  ,\n",
       "         20.  , 23.  , 50.  ,   nan,  3.  ,   nan, 37.  , 28.  ,   nan,\n",
       "         39.  , 38.5 ,   nan,   nan])>,\n",
       "  'SibSp': <tf.Tensor: shape=(418,), dtype=int64, numpy=\n",
       "  array([0, 1, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 2, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 1, 0, 2, 3, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 5, 0, 1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 8, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 1, 1, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 2, 0, 0, 1, 8, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "         1, 0, 2, 0, 0, 4, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 3, 1,\n",
       "         0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "        dtype=int64)>,\n",
       "  'Parch': <tf.Tensor: shape=(418,), dtype=int64, numpy=\n",
       "  array([0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 3, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         4, 0, 0, 0, 0, 0, 0, 6, 2, 0, 3, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2,\n",
       "         2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 5, 2,\n",
       "         0, 0, 3, 2, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 2, 0,\n",
       "         0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2,\n",
       "         1, 0, 2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 9, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 2, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 9, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        dtype=int64)>,\n",
       "  'Fare': <tf.Tensor: shape=(418,), dtype=float64, numpy=\n",
       "  array([  7.8292,   7.    ,   9.6875,   8.6625,  12.2875,   9.225 ,\n",
       "           7.6292,  29.    ,   7.2292,  24.15  ,   7.8958,  26.    ,\n",
       "          82.2667,  26.    ,  61.175 ,  27.7208,  12.35  ,   7.225 ,\n",
       "           7.925 ,   7.225 ,  59.4   ,   3.1708,  31.6833,  61.3792,\n",
       "         262.375 ,  14.5   ,  61.9792,   7.225 ,  30.5   ,  21.6792,\n",
       "          26.    ,  31.5   ,  20.575 ,  23.45  ,  57.75  ,   7.2292,\n",
       "           8.05  ,   8.6625,   9.5   ,  56.4958,  13.4167,  26.55  ,\n",
       "           7.85  ,  13.    ,  52.5542,   7.925 ,  29.7   ,   7.75  ,\n",
       "          76.2917,  15.9   ,  60.    ,  15.0333,  23.    , 263.    ,\n",
       "          15.5792,  29.125 ,   7.8958,   7.65  ,  16.1   , 262.375 ,\n",
       "           7.8958,  13.5   ,   7.75  ,   7.725 , 262.375 ,  21.    ,\n",
       "           7.8792,  42.4   ,  28.5375, 263.    ,   7.75  ,   7.8958,\n",
       "           7.925 ,  27.7208, 211.5   , 211.5   ,   8.05  ,  25.7   ,\n",
       "          13.    ,   7.75  ,  15.2458, 221.7792,  26.    ,   7.8958,\n",
       "          10.7083,  14.4542,   7.8792,   8.05  ,   7.75  ,  23.    ,\n",
       "          13.9   ,   7.775 ,  52.    ,   8.05  ,  26.    ,   7.7958,\n",
       "          78.85  ,   7.925 ,   7.8542,   8.05  ,  55.4417,  26.    ,\n",
       "           7.75  ,   7.775 ,   8.5167,  22.525 ,   7.8208,   7.75  ,\n",
       "           8.7125,  13.    ,  15.0458,   7.7792,  31.6792,   7.2833,\n",
       "         221.7792,  14.4542,   6.4375,  16.7   ,  75.2417,  26.    ,\n",
       "          15.75  ,   7.75  ,  57.75  ,   7.25  ,   7.75  ,  16.1   ,\n",
       "           7.7958,  23.25  ,  13.    ,   8.05  ,   8.05  ,  28.5   ,\n",
       "          25.4667,   6.4375,   7.8958,   7.8542,   7.225 ,  13.    ,\n",
       "           8.05  ,  46.9   ,  46.9   , 151.55  , 262.375 ,  26.    ,\n",
       "          26.55  ,  18.    ,  51.8625,   8.05  ,  26.55  ,  26.    ,\n",
       "          83.1583,   7.8958,      nan,  12.1833,  31.3875,   7.55  ,\n",
       "         221.7792,   7.8542,  26.55  ,  13.775 ,   7.7333,  15.2458,\n",
       "          13.5   ,   7.    ,  13.    ,  22.025 ,  50.4958,  34.375 ,\n",
       "          27.7208,   8.9625,   7.55  ,   7.225 ,  13.9   ,   7.2292,\n",
       "          31.3875,  39.    ,  36.75  ,  55.4417,  39.    ,  83.1583,\n",
       "          13.    ,  83.1583,  53.1   ,   7.75  , 247.5208,  16.    ,\n",
       "          21.    ,   8.05  ,  69.55  ,  13.    ,  26.    ,  26.    ,\n",
       "          14.5   ,  12.35  ,  32.5   ,   7.8542, 134.5   ,   7.775 ,\n",
       "          10.5   ,   8.1125,  15.5   ,  14.4   , 227.525 ,  26.    ,\n",
       "          10.5   ,  25.7417,   7.75  ,  10.5   ,  27.7208,   7.8958,\n",
       "          22.525 ,   7.05  ,  73.5   ,  26.    ,   7.775 ,  42.5   ,\n",
       "           7.8792, 164.8667, 211.5   ,   8.05  ,  13.8583,   8.05  ,\n",
       "          10.5   ,   7.7958,  27.4458,  15.2458,   7.7958,   7.75  ,\n",
       "          15.1   ,  13.    ,  65.    ,  26.55  ,   6.4958,   7.8792,\n",
       "          71.2833,   7.8542,  75.25  ,   7.225 ,  13.    , 106.425 ,\n",
       "          27.7208,  30.    , 134.5   ,   7.8875,  23.45  ,  51.8625,\n",
       "          21.    ,  32.5   ,  26.    ,  14.4542,  27.75  ,   7.925 ,\n",
       "         136.7792,   9.325 ,   9.5   ,   7.55  ,   7.75  ,   8.05  ,\n",
       "          13.    ,   7.775 ,  17.4   ,   7.8542,  23.    ,  12.1833,\n",
       "          12.7375,   7.8958,   0.    ,   7.55  ,   8.05  ,   8.6625,\n",
       "          75.2417,   7.75  , 136.7792,  15.5   ,   7.225 ,  26.    ,\n",
       "          10.5   ,  26.    ,  21.    ,  10.5   ,   8.6625,  13.775 ,\n",
       "           7.75  ,  15.2458,  20.2125,   7.25  ,   7.25  ,  82.2667,\n",
       "           7.2292,   8.05  ,  39.6   ,   6.95  ,   7.2292,  81.8583,\n",
       "           9.5   ,   7.8958,  41.5792,  21.6792,  45.5   ,   7.8542,\n",
       "           7.775 ,  15.0458,  21.    ,   8.6625,   7.75  ,  26.55  ,\n",
       "         151.55  ,   9.35  ,  93.5   ,  14.1083,   8.6625,   7.225 ,\n",
       "           7.575 ,   7.75  , 135.6333,   7.7333, 146.5208,  10.5   ,\n",
       "           7.8542,  31.5   ,   7.775 ,   7.2292,  13.    ,  26.55  ,\n",
       "         211.3375,   7.05  ,  39.    ,  79.2   ,  26.    ,  13.    ,\n",
       "          36.75  ,  29.7   ,   7.225 ,  15.7417,   7.8958,  26.    ,\n",
       "          13.    ,   7.2292,  31.5   ,   7.2292,  10.5   ,   7.5792,\n",
       "          69.55  , 512.3292,  14.5   ,   7.65  ,  13.    ,   7.2292,\n",
       "          13.5   ,  21.    ,  63.3583,  10.5   ,  73.5   ,  65.    ,\n",
       "          20.575 ,  26.    ,  51.4792,   7.8792,   7.75  ,  15.55  ,\n",
       "          69.55  ,  37.0042,  21.    ,   8.6625,  55.4417,  69.55  ,\n",
       "          14.4583,  39.6875,  59.4   ,  13.8583,  11.5   , 134.5   ,\n",
       "           0.    ,  13.    ,  81.8583, 262.375 ,   8.6625,  11.5   ,\n",
       "          50.    ,  31.3875,   7.75  ,   7.8792,  14.5   ,  16.1   ,\n",
       "          12.875 ,  65.    ,   7.775 ,  13.    ,   7.75  ,  21.075 ,\n",
       "          93.5   ,  39.4   ,  20.25  ,  10.5   ,  22.025 ,  60.    ,\n",
       "           7.25  ,  79.2   ,   7.775 ,   7.7333, 164.8667,  21.    ,\n",
       "          59.4   ,  47.1   ,  27.7208,  13.8625,  10.5   , 211.5   ,\n",
       "           7.7208,  13.775 ,   7.75  ,  90.    ,   7.775 ,   8.05  ,\n",
       "         108.9   ,   7.25  ,   8.05  ,  22.3583])>,\n",
       "  'Embarked': <tf.Tensor: shape=(418,), dtype=string, numpy=\n",
       "  array([b'Q', b'S', b'Q', b'S', b'S', b'S', b'Q', b'S', b'C', b'S', b'S',\n",
       "         b'S', b'S', b'S', b'S', b'C', b'Q', b'C', b'S', b'C', b'C', b'S',\n",
       "         b'S', b'C', b'C', b'S', b'C', b'C', b'S', b'C', b'S', b'S', b'S',\n",
       "         b'S', b'C', b'C', b'S', b'S', b'S', b'S', b'C', b'S', b'S', b'S',\n",
       "         b'S', b'S', b'C', b'Q', b'C', b'S', b'S', b'C', b'S', b'S', b'C',\n",
       "         b'Q', b'S', b'S', b'S', b'C', b'S', b'S', b'S', b'Q', b'C', b'S',\n",
       "         b'Q', b'S', b'C', b'S', b'Q', b'S', b'S', b'C', b'C', b'C', b'S',\n",
       "         b'S', b'S', b'Q', b'C', b'S', b'S', b'S', b'Q', b'C', b'Q', b'S',\n",
       "         b'Q', b'S', b'S', b'S', b'S', b'S', b'C', b'S', b'S', b'S', b'S',\n",
       "         b'S', b'C', b'S', b'Q', b'S', b'C', b'S', b'Q', b'Q', b'S', b'S',\n",
       "         b'C', b'Q', b'C', b'Q', b'S', b'C', b'C', b'S', b'C', b'S', b'S',\n",
       "         b'Q', b'C', b'S', b'Q', b'S', b'S', b'Q', b'S', b'S', b'S', b'C',\n",
       "         b'S', b'C', b'S', b'S', b'C', b'S', b'S', b'S', b'S', b'S', b'C',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'S', b'C', b'C', b'S', b'S',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'Q', b'C', b'S', b'S', b'S',\n",
       "         b'S', b'C', b'S', b'C', b'S', b'S', b'C', b'S', b'C', b'S', b'S',\n",
       "         b'S', b'C', b'S', b'C', b'S', b'C', b'S', b'Q', b'C', b'S', b'S',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'Q', b'S', b'S', b'C', b'S',\n",
       "         b'S', b'S', b'Q', b'S', b'C', b'S', b'S', b'C', b'Q', b'S', b'C',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'S', b'Q', b'S', b'C', b'S',\n",
       "         b'C', b'S', b'S', b'S', b'C', b'C', b'S', b'Q', b'S', b'S', b'S',\n",
       "         b'S', b'S', b'Q', b'C', b'S', b'C', b'C', b'S', b'C', b'C', b'S',\n",
       "         b'C', b'S', b'S', b'S', b'S', b'S', b'S', b'C', b'S', b'S', b'C',\n",
       "         b'S', b'S', b'S', b'Q', b'S', b'S', b'S', b'S', b'S', b'S', b'S',\n",
       "         b'C', b'S', b'S', b'S', b'S', b'S', b'C', b'Q', b'C', b'Q', b'C',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'S', b'Q', b'C', b'S', b'S',\n",
       "         b'S', b'S', b'C', b'S', b'S', b'Q', b'C', b'S', b'S', b'S', b'C',\n",
       "         b'C', b'S', b'S', b'S', b'C', b'S', b'S', b'Q', b'S', b'S', b'S',\n",
       "         b'S', b'S', b'S', b'C', b'S', b'Q', b'C', b'Q', b'C', b'S', b'S',\n",
       "         b'S', b'S', b'C', b'S', b'S', b'S', b'S', b'S', b'C', b'S', b'S',\n",
       "         b'S', b'C', b'C', b'C', b'S', b'S', b'S', b'C', b'S', b'C', b'S',\n",
       "         b'S', b'S', b'C', b'S', b'S', b'S', b'C', b'S', b'S', b'C', b'S',\n",
       "         b'S', b'S', b'S', b'S', b'S', b'S', b'Q', b'S', b'S', b'C', b'S',\n",
       "         b'S', b'C', b'S', b'C', b'S', b'C', b'C', b'S', b'C', b'S', b'S',\n",
       "         b'S', b'C', b'S', b'S', b'S', b'S', b'Q', b'Q', b'S', b'S', b'S',\n",
       "         b'S', b'S', b'S', b'Q', b'S', b'S', b'S', b'S', b'S', b'S', b'S',\n",
       "         b'Q', b'C', b'S', b'Q', b'S', b'S', b'C', b'S', b'C', b'C', b'S',\n",
       "         b'C', b'Q', b'S', b'Q', b'Q', b'S', b'S', b'C', b'S', b'S', b'C'],\n",
       "        dtype=object)>}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(titanic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = kr.models.load_model(filepath=inference_model_path)\n",
    "training_model = kr.models.load_model(filepath=training_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([3], dtype=int64)>,\n",
       " 'Sex': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'female'], dtype=object)>,\n",
       " 'Age': <tf.Tensor: shape=(1,), dtype=float64, numpy=array([27.])>,\n",
       " 'SibSp': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1], dtype=int64)>,\n",
       " 'Parch': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0], dtype=int64)>,\n",
       " 'Fare': <tf.Tensor: shape=(1,), dtype=float64, numpy=array([7.925])>,\n",
       " 'Embarked': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'S'], dtype=object)>}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = titanic_data_test.sample(n=1)\n",
    "input_dict = {name: tf.convert_to_tensor(value) for name, value in sample.items()}\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.61963636]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = inference_model.predict(input_dict)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.02990931],\n",
       "       [0.3144216 ],\n",
       "       [0.02304282],\n",
       "       [0.17936237],\n",
       "       [0.5197228 ],\n",
       "       [0.12228212],\n",
       "       [0.31785667],\n",
       "       [0.05304518],\n",
       "       [0.6795259 ],\n",
       "       [0.03222797],\n",
       "       [0.12220655],\n",
       "       [0.35094997],\n",
       "       [0.9620445 ],\n",
       "       [0.03737288],\n",
       "       [0.9747287 ],\n",
       "       [0.8724473 ],\n",
       "       [0.022188  ],\n",
       "       [0.2841512 ],\n",
       "       [0.61963636],\n",
       "       [0.43821785],\n",
       "       [0.68195593],\n",
       "       [0.56051874],\n",
       "       [0.9909534 ],\n",
       "       [0.08949408],\n",
       "       [0.9729398 ],\n",
       "       [0.05127372],\n",
       "       [0.9786322 ],\n",
       "       [0.2841512 ],\n",
       "       [0.33803028],\n",
       "       [0.23229979],\n",
       "       [0.03737288],\n",
       "       [0.03076835],\n",
       "       [0.5522429 ],\n",
       "       [0.4060215 ],\n",
       "       [0.64331573],\n",
       "       [0.2841488 ],\n",
       "       [0.36350477],\n",
       "       [0.54250395],\n",
       "       [0.18162231],\n",
       "       [0.13529949],\n",
       "       [0.14593613],\n",
       "       [0.31396914],\n",
       "       [0.07421533],\n",
       "       [0.9752322 ],\n",
       "       [0.97195244],\n",
       "       [0.1773906 ],\n",
       "       [0.30003637],\n",
       "       [0.06833693],\n",
       "       [0.9683054 ],\n",
       "       [0.27537373],\n",
       "       [0.8530724 ],\n",
       "       [0.10497151],\n",
       "       [0.9478614 ],\n",
       "       [0.9991668 ],\n",
       "       [0.19495864],\n",
       "       [0.04339106],\n",
       "       [0.07419709],\n",
       "       [0.1766597 ],\n",
       "       [0.18626787],\n",
       "       [0.99467415],\n",
       "       [0.12220655],\n",
       "       [0.09349117],\n",
       "       [0.07365532],\n",
       "       [0.46029338],\n",
       "       [0.6966543 ],\n",
       "       [0.9842024 ],\n",
       "       [0.4604545 ],\n",
       "       [0.27636954],\n",
       "       [0.37273896],\n",
       "       [0.80400205],\n",
       "       [0.7800542 ],\n",
       "       [0.07371368],\n",
       "       [0.55826145],\n",
       "       [0.36870044],\n",
       "       [0.992084  ],\n",
       "       [0.90300053],\n",
       "       [0.12221698],\n",
       "       [0.9292436 ],\n",
       "       [0.09313286],\n",
       "       [0.7800542 ],\n",
       "       [0.5619972 ],\n",
       "       [0.3293215 ],\n",
       "       [0.35094997],\n",
       "       [0.12220655],\n",
       "       [0.08792222],\n",
       "       [0.16673678],\n",
       "       [0.7800225 ],\n",
       "       [0.54230195],\n",
       "       [0.9116349 ],\n",
       "       [0.9488244 ],\n",
       "       [0.39249915],\n",
       "       [0.12219018],\n",
       "       [0.97685736],\n",
       "       [0.12221698],\n",
       "       [0.86004823],\n",
       "       [0.17704692],\n",
       "       [0.98084927],\n",
       "       [0.3243331 ],\n",
       "       [0.5422373 ],\n",
       "       [0.32487252],\n",
       "       [0.97441804],\n",
       "       [0.04751126],\n",
       "       [0.06833693],\n",
       "       [0.17699163],\n",
       "       [0.7574198 ],\n",
       "       [0.37870377],\n",
       "       [0.03033531],\n",
       "       [0.06833693],\n",
       "       [0.12225367],\n",
       "       [0.07457212],\n",
       "       [0.09890325],\n",
       "       [0.9116095 ],\n",
       "       [0.92800975],\n",
       "       [0.45983168],\n",
       "       [0.9972637 ],\n",
       "       [0.20467432],\n",
       "       [0.10514472],\n",
       "       [0.6905775 ],\n",
       "       [0.19624415],\n",
       "       [0.9869133 ],\n",
       "       [0.9847217 ],\n",
       "       [0.07574843],\n",
       "       [0.9756003 ],\n",
       "       [0.3214278 ],\n",
       "       [0.06833693],\n",
       "       [0.7281551 ],\n",
       "       [0.07367367],\n",
       "       [0.867385  ],\n",
       "       [0.05523799],\n",
       "       [0.1777236 ],\n",
       "       [0.32487252],\n",
       "       [0.30376974],\n",
       "       [0.09721528],\n",
       "       [0.16843353],\n",
       "       [0.07419709],\n",
       "       [0.17720218],\n",
       "       [0.32614267],\n",
       "       [0.07284082],\n",
       "       [0.75600606],\n",
       "       [0.00447447],\n",
       "       [0.15900606],\n",
       "       [0.9964564 ],\n",
       "       [0.01060482],\n",
       "       [0.10285816],\n",
       "       [0.35037363],\n",
       "       [0.04454632],\n",
       "       [0.29349408],\n",
       "       [0.07377546],\n",
       "       [0.31396914],\n",
       "       [0.19407585],\n",
       "       [0.9558134 ],\n",
       "       [0.10549946],\n",
       "       [0.5723493 ],\n",
       "       [0.27251932],\n",
       "       [0.05680207],\n",
       "       [0.17639455],\n",
       "       [0.9986146 ],\n",
       "       [0.7556877 ],\n",
       "       [0.35037363],\n",
       "       [0.89515716],\n",
       "       [0.91164947],\n",
       "       [0.5619972 ],\n",
       "       [0.75332665],\n",
       "       [0.12208534],\n",
       "       [0.05523799],\n",
       "       [0.7852208 ],\n",
       "       [0.23799248],\n",
       "       [0.03528026],\n",
       "       [0.98391134],\n",
       "       [0.54260284],\n",
       "       [0.12215972],\n",
       "       [0.32614267],\n",
       "       [0.07860513],\n",
       "       [0.10533631],\n",
       "       [0.01027326],\n",
       "       [0.9850915 ],\n",
       "       [0.9517525 ],\n",
       "       [0.66656214],\n",
       "       [0.8011056 ],\n",
       "       [0.97322583],\n",
       "       [0.09313286],\n",
       "       [0.4640835 ],\n",
       "       [0.9804029 ],\n",
       "       [0.06833693],\n",
       "       [0.99348664],\n",
       "       [0.05427611],\n",
       "       [0.98067755],\n",
       "       [0.1666282 ],\n",
       "       [0.06529884],\n",
       "       [0.05523799],\n",
       "       [0.03737288],\n",
       "       [0.31413525],\n",
       "       [0.58650255],\n",
       "       [0.022188  ],\n",
       "       [0.9840498 ],\n",
       "       [0.32402775],\n",
       "       [0.2400994 ],\n",
       "       [0.54221123],\n",
       "       [0.07200073],\n",
       "       [0.36366475],\n",
       "       [0.9042947 ],\n",
       "       [0.80233526],\n",
       "       [0.7240411 ],\n",
       "       [0.99310136],\n",
       "       [0.07200073],\n",
       "       [0.13898888],\n",
       "       [0.25949216],\n",
       "       [0.07200073],\n",
       "       [0.9878284 ],\n",
       "       [0.17731293],\n",
       "       [0.37870377],\n",
       "       [0.12209211],\n",
       "       [0.16787422],\n",
       "       [0.83609796],\n",
       "       [0.13430063],\n",
       "       [0.27590743],\n",
       "       [0.9115223 ],\n",
       "       [0.05742176],\n",
       "       [0.99298   ],\n",
       "       [0.12221698],\n",
       "       [0.98779684],\n",
       "       [0.07377546],\n",
       "       [0.96371263],\n",
       "       [0.07367367],\n",
       "       [0.92611575],\n",
       "       [0.54502285],\n",
       "       [0.17704692],\n",
       "       [0.9116349 ],\n",
       "       [0.07139218],\n",
       "       [0.05523799],\n",
       "       [0.16959599],\n",
       "       [0.98095584],\n",
       "       [0.05696825],\n",
       "       [0.06830978],\n",
       "       [0.712786  ],\n",
       "       [0.07369703],\n",
       "       [0.71093106],\n",
       "       [0.2841512 ],\n",
       "       [0.9718842 ],\n",
       "       [0.98983586],\n",
       "       [0.9262409 ],\n",
       "       [0.95305395],\n",
       "       [0.18579884],\n",
       "       [0.12220537],\n",
       "       [0.5437151 ],\n",
       "       [0.3256416 ],\n",
       "       [0.9604732 ],\n",
       "       [0.04855742],\n",
       "       [0.9869133 ],\n",
       "       [0.5865549 ],\n",
       "       [0.9907943 ],\n",
       "       [0.07372536],\n",
       "       [0.87843597],\n",
       "       [0.18114825],\n",
       "       [0.33109325],\n",
       "       [0.12215972],\n",
       "       [0.06833693],\n",
       "       [0.32487252],\n",
       "       [0.9631238 ],\n",
       "       [0.07366531],\n",
       "       [0.05029393],\n",
       "       [0.07369703],\n",
       "       [0.9327246 ],\n",
       "       [0.6832664 ],\n",
       "       [0.11074364],\n",
       "       [0.12220655],\n",
       "       [0.31461155],\n",
       "       [0.12215972],\n",
       "       [0.36350477],\n",
       "       [0.12225091],\n",
       "       [0.19624415],\n",
       "       [0.06833693],\n",
       "       [0.98513883],\n",
       "       [0.94495904],\n",
       "       [0.10533528],\n",
       "       [0.9189257 ],\n",
       "       [0.09135986],\n",
       "       [0.03737288],\n",
       "       [0.13818185],\n",
       "       [0.07687405],\n",
       "       [0.7570273 ],\n",
       "       [0.585767  ],\n",
       "       [0.9116349 ],\n",
       "       [0.77007735],\n",
       "       [0.694623  ],\n",
       "       [0.07445487],\n",
       "       [0.12211913],\n",
       "       [0.8616984 ],\n",
       "       [0.10533631],\n",
       "       [0.12221698],\n",
       "       [0.30550784],\n",
       "       [0.31815982],\n",
       "       [0.10533631],\n",
       "       [0.20110953],\n",
       "       [0.07356034],\n",
       "       [0.17731293],\n",
       "       [0.9875815 ],\n",
       "       [0.23229979],\n",
       "       [0.45583993],\n",
       "       [0.32402775],\n",
       "       [0.3236863 ],\n",
       "       [0.19519724],\n",
       "       [0.08096162],\n",
       "       [0.17936237],\n",
       "       [0.9116349 ],\n",
       "       [0.9815222 ],\n",
       "       [0.4502958 ],\n",
       "       [0.5782326 ],\n",
       "       [0.15405437],\n",
       "       [0.3312223 ],\n",
       "       [0.07401884],\n",
       "       [0.2841512 ],\n",
       "       [0.1221631 ],\n",
       "       [0.25949216],\n",
       "       [0.98697984],\n",
       "       [0.91164947],\n",
       "       [0.62696815],\n",
       "       [0.07687405],\n",
       "       [0.17720218],\n",
       "       [0.05123776],\n",
       "       [0.17699163],\n",
       "       [0.32615617],\n",
       "       [0.07284082],\n",
       "       [0.39541402],\n",
       "       [0.9959282 ],\n",
       "       [0.17507328],\n",
       "       [0.9898496 ],\n",
       "       [0.19266875],\n",
       "       [0.14792015],\n",
       "       [0.07457212],\n",
       "       [0.95374864],\n",
       "       [0.30003637],\n",
       "       [0.10533528],\n",
       "       [0.8955232 ],\n",
       "       [0.17731293],\n",
       "       [0.3933077 ],\n",
       "       [0.09313286],\n",
       "       [0.16775087],\n",
       "       [0.08003975],\n",
       "       [0.10533631],\n",
       "       [0.07687405],\n",
       "       [0.322843  ],\n",
       "       [0.00453798],\n",
       "       [0.99862516],\n",
       "       [0.58650255],\n",
       "       [0.3624816 ],\n",
       "       [0.07284082],\n",
       "       [0.4382157 ],\n",
       "       [0.07300994],\n",
       "       [0.97716165],\n",
       "       [0.9633204 ],\n",
       "       [0.07200073],\n",
       "       [0.04058167],\n",
       "       [0.04475936],\n",
       "       [0.39706472],\n",
       "       [0.35094997],\n",
       "       [0.94559973],\n",
       "       [0.12220424],\n",
       "       [0.06833693],\n",
       "       [0.5124249 ],\n",
       "       [0.00939388],\n",
       "       [0.96786445],\n",
       "       [0.97716165],\n",
       "       [0.17936237],\n",
       "       [0.9602293 ],\n",
       "       [0.00329112],\n",
       "       [0.16673593],\n",
       "       [0.57962275],\n",
       "       [0.96172357],\n",
       "       [0.11215664],\n",
       "       [0.06426577],\n",
       "       [0.9977103 ],\n",
       "       [0.3017934 ],\n",
       "       [0.05523799],\n",
       "       [0.98461807],\n",
       "       [0.99467415],\n",
       "       [0.27502692],\n",
       "       [0.07597776],\n",
       "       [0.2438203 ],\n",
       "       [0.05680207],\n",
       "       [0.06833693],\n",
       "       [0.04459596],\n",
       "       [0.38160184],\n",
       "       [0.3905079 ],\n",
       "       [0.1999101 ],\n",
       "       [0.97504175],\n",
       "       [0.17699163],\n",
       "       [0.05523799],\n",
       "       [0.03035359],\n",
       "       [0.11711147],\n",
       "       [0.68776363],\n",
       "       [0.9888947 ],\n",
       "       [0.8060676 ],\n",
       "       [0.0559787 ],\n",
       "       [0.03675796],\n",
       "       [0.9814892 ],\n",
       "       [0.04456925],\n",
       "       [0.98411775],\n",
       "       [0.07366531],\n",
       "       [0.33252358],\n",
       "       [0.9970845 ],\n",
       "       [0.03828912],\n",
       "       [0.97868615],\n",
       "       [0.29818973],\n",
       "       [0.5097109 ],\n",
       "       [0.18176807],\n",
       "       [0.04113915],\n",
       "       [0.17200197],\n",
       "       [0.9116603 ],\n",
       "       [0.68637246],\n",
       "       [0.9116349 ],\n",
       "       [0.9445617 ],\n",
       "       [0.55765647],\n",
       "       [0.12221698],\n",
       "       [0.9831079 ],\n",
       "       [0.07445487],\n",
       "       [0.12221698],\n",
       "       [0.5688572 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = inference_model.predict(x=titanic_dataset)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titanic-ml-tfpj-M2nteehU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
